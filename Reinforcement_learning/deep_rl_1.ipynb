{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习实验\n",
    "强化学习是一个理论与实践相结合的机器学习分支，我们不仅要理解它算法背后的一些数学原理，还要通过上机实践实现算法。在很多实验环境里面去探索算法能不能得到预期效果也是一个非常重要的过程。\n",
    "\n",
    "我们可以使用 Python 和深度学习的一些包来实现强化学习算法。现在有很多深度学习的包可以使用，比如 PyTorch、TensorFlow、Keras，熟练使用其中的两三种，就可以实现非常多的功能。所以我们并不需要从头去“造轮子”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym\n",
    "OpenAI是一家非营利性的人工智能研究公司，其公布了非常多的学习资源以及算法资源。其之所以叫作 OpenAI，他们把所有开发的算法都进行了开源。 如图 1.24 所示，OpenAI 的 Gym库是一个环境仿真库，里面包含很多现有的环境。针对不同的场景，我们可以选择不同的环境。离散控制场景（输出的动作是可数的，比如Pong游戏中输出的向上或向下动作）一般使用雅达利环境评估；连续控制场景（输出的动作是不可数的，比如机器人走路时不仅有方向，还有角度，角度就是不可数的，是一个连续的量 ）一般使用 MuJoCo 环境评估。Gym Retro是对 Gym 环境的进一步扩展，包含更多的游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过 pip 来安装 Gym 库，由于 Gym 库 0.26.0 及其之后的版本对之前的代码不兼容，所以我们安装 0.26.0 之前的 Gym，比如 0.25.2。\n",
    "`pip install gym==0.25.2`\n",
    "此外，为了显示图形界面，我们还需要安装 pygame 库。\n",
    "`pip install pygame`\n",
    "\n",
    "注意：建议大家在minicoda中创建一个专门的虚拟环境来运行这个项目\n",
    "\n",
    "比如我们现在安装了Gym库，就可以直接调入Taxi-v3的环境。初始化这个环境后，我们就可以进行交互了。智能体得到某个观测后，它就会输出一个动作。这个动作会被环境拿去执行某个步骤 ，然后环境就会往前走一步，返回新的观测、奖励以及一个 flag 变量 done，done 决定这个游戏是不是结束了。我们通过几行代码就可实现强化学习的框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "env = gym.make(\"Taxi-v3\") \n",
    "observation = env.reset() \n",
    "agent = load_agent() \n",
    "for step in range(100):\n",
    "    action = agent(observation) \n",
    "    observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这段代码只是示例，其目的是让读者了解强化学习算法代码实现的框架，并非完整代码，load_agent 函数并未定义，所以运行这段代码会报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图 1.25 所示，Gym 库里面有很多经典的控制类游戏。比如 Acrobot需要让一个双连杆机器人立起来；CartPole需要通过控制一辆小车，让杆立起来；MountainCar需要通过前后移动车，让它到达旗帜的位置。在刚开始测试强化学习的时候，我们可以选择这些简单环境，因为强化学习在这些环境中可以在一两分钟之内见到效果。\n",
    "<div align=\"center\">\n",
    "<img src=\"https://datawhalechina.github.io/easy-rl/img/ch1/1.46.png\" alt=\"经典控制问题\" width=\"400\" >\n",
    "</div>\n",
    "大家可以点[这个链接](https://www.gymlibrary.dev/environments/classic_control/)看一看这些环境。在刚开始测试强化学习的时候，可以选择这些简单环境，因为这些环境可以在一两分钟之内见到一个效果。\n",
    "\n",
    "如图所示，CartPole-v0 环境有两个动作：将小车向左移动和将小车向右移动。我们还可以得到观测：小车当前的位置，小车当前往左、往右移的速度，杆的角度以及杆的最高点（顶端）的速度。 观测越详细，我们就可以更好地描述当前所有的状态。这里有奖励的定义，如果能多走一步，我们就会得到一个奖励（奖励值为1），所以我们需要存活尽可能多的时间来得到更多的奖励。当杆的角度大于某一个角度（没能保持平衡），或者小车的中心到达图形界面窗口的边缘，或者累积步数大于200，游戏就结束了，我们就输了。所以智能体的目的是控制杆，让它尽可能地保持平衡以及尽可能保持在环境的中央。\n",
    "<div align=\"center\">\n",
    "<img src=\"https://datawhalechina.github.io/easy-rl/img/ch1/1.47.png\" alt=\"经典控制问题\" width=\"400\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # 导入 Gym 的 Python 接口环境包\n",
    "env = gym.make('CartPole-v0')  # 构建实验环境\n",
    "env.reset()  # 重置一个回合\n",
    "for _ in range(1000):\n",
    "    env.render()  # 显示图形界面\n",
    "    action = env.action_space.sample() # 从动作空间中随机选取一个动作\n",
    "    env.step(action) # 用于提交动作，括号内是具体的动作\n",
    "env.close() # 关闭环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：如果绘制了实验的图形界面窗口，那么关闭该窗口的最佳方式是调用 env.close()。试图直接关闭图形界面窗口可能会导致内存不能释放，甚至会导致死机。\n",
    "\n",
    "当我们执行这段代码时，机器人会驾驶着小车朝某个方向一直跑，直到我们看不见，这是因为我们还没开始训练机器人。\n",
    "\n",
    "Gym 库中的大部分小游戏都可以用一个普通的实数或者向量来表示动作。输出env.action_space.sample()的返回值，能看到输出为 1 或者 0。env.action_space.sample()的含义是，在该游戏的所有动作空间里随机选择一个作为输出。在这个例子中，动作只有两个：0 和 1，一左一右。env.step()方法有4个返回值：observation、reward、done、info 。\n",
    "\n",
    "observation 是状态信息，是在游戏中观测到的屏幕像素值或者盘面状态描述信息。reward 是奖励值，即动作提交以后能够获得的奖励值。这个奖励值因游戏的不同而不同，但总体原则是，对完成游戏有帮助的动作会获得比较高的奖励值。done 表示游戏是否已经完成，如果完成了，就需要重置游戏并开始一个新的回合。info 是一些比较原始的用于诊断和调试的信息，或许对训练有帮助。不过，OpenAI 在评价我们提交的机器人时，是不允许使用这些信息的。\n",
    "\n",
    "在每个训练中都要使用的返回值有 observation、reward、done。但 observation 的结构会由于游戏的不同而发生变化。以 CartPole-v0 为例，我们对代码进行修改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  \n",
    "env = gym.make('CartPole-v0')  \n",
    "env.reset()  \n",
    "for _ in range(1000):\n",
    "    env.render()  \n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(observation)\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从输出可以看出这是一个四维的观测。在其他游戏中会有维度更多的情况出现。\n",
    "\n",
    "env.step()完成了一个完整的S→A→R→S过程。我们只要不断观测这样的过程，并让智能体在其中用相应的算法完成训练，就能得到一个高质量的强化学习模型。\n",
    "\n",
    "Gym 库已注册的环境可以通过以下代码查看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "env_specs = envs.registry.all()\n",
    "envs_ids = [env_spec.id for env_spec in env_specs]\n",
    "print(envs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym 库中的每个环境都定义了观测空间和动作空间。观测空间和动作空间可以是离散的（取值为有限个离散的值），也可以是连续的（取值为连续的值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar-v0 例子\n",
    "接下来，我们通过一个例子来学习如何与 Gym 库进行交互。我们选取小车上山（MountainCar-v0）作为例子。\n",
    "\n",
    "首先我们来看看这个任务的观测空间和动作空间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('观测范围 = {} ~ {}'.format(env.observation_space.low,\n",
    "        env.observation_space.high))\n",
    "print('动作数 = {}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Gym 库中，环境的观测空间用 env.observation_space 表示，动作空间用 env.action_space 表示。离散空间 gym.spaces.Discrete 类表示，连续空间用 gym.spaces.Box 类表示。对于离散空间，Discrete (n) 表示可能取值的数量为 n；对于连续空间，Box类实例成员中的 low 和 high 表示每个浮点数的取值范围。MountainCar-v0 中的观测是长度为 2 的 numpy 数组，数组中值的类型为 float。MountainCar-v0 中的动作是整数，取值范围为 {0,1,2}。\n",
    "\n",
    "接下来实现智能体来控制小车移动，对应代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def decide(self, observation): # 决策\n",
    "        position, velocity = observation\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "        return action # 返回动作\n",
    "\n",
    "    def learn(self, *args): # 学习\n",
    "        pass\n",
    "    \n",
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleAgent 类的 decide()方法用于决策，learn() 方法用于学习，该智能体不是强化学习智能体，不能学习，只能根据给定的数学表达式进行决策。\n",
    "\n",
    "接下来我们试图让智能体与环境交互，代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, agent, render=False, train=False):\n",
    "    episode_reward = 0. # 记录回合总奖励，初始值为0\n",
    "    observation = env.reset() # 重置游戏环境，开始新回合\n",
    "    while True: # 不断循环，直到回合结束\n",
    "        if render: # 判断是否显示\n",
    "            env.render() # 显示图形界面\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action) # 执行动作\n",
    "        episode_reward += reward # 收集回合奖励\n",
    "        if train: # 判断是否训练智能体\n",
    "            agent.learn(observation, action, reward, done) # 学习\n",
    "        if done: # 回合结束，跳出循环\n",
    "            break\n",
    "        observation = next_observation\n",
    "    return episode_reward # 返回回合总奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码中的 play 函数可以让智能体和环境交互一个回合，该函数有 4 个参数。env 是环境类。agent 是智能体类。render 是 bool 型变量，其用于判断是否需要图形化显示。如果 render 为 True，则在交互过程中会调用 env.render() 以显示图形界面，通过调用 env.close() 可关闭图形界面。train 是 bool 型变量，其用于判断是否训练智能体，在训练过程中设置为 True，让智能体学习；在测试过程中设置为 False，让智能体保持不变。该函数的返回值 episode_reward 是 float 型的数值，其表示智能体与环境交互一个回合的回合总奖励。\n",
    "\n",
    "接下来，我们使用下面的代码让智能体和环境交互一个回合，并显示图形界面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(3) # 设置随机种子，让结果可复现\n",
    "episode_reward = play(env, agent, render=True)\n",
    "print('回合奖励 = {}'.format(episode_reward))\n",
    "env.close() # 关闭图形界面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励，代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "episode_rewards = [play(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {}'.format(np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleAgent 类对应策略的平均回合奖励在 110 左右，而对于小车上山任务，只要连续 100 个回合的平均回合奖励大于110，就可以认为该任务被解决了。完整代码实现如下\n",
    "\n",
    "测试智能体在 Gym 库中某个任务的性能时，出于习惯使然，学术界一般最关心 100 个回合的平均回合奖励。对于有些任务，还会指定一个参考的回合奖励值，当连续 100 个回合的奖励大于指定的值时，则认为该任务被解决了。而对于没有指定值的任务，就无所谓任务被解决了或没有被解决。\n",
    "\n",
    "我们对 Gym 库的用法进行总结：使用 env=gym.make(环境名)取出环境，使用 env.reset()初始化环境，使用 env.step(动作)执行一步环境，使用 env.render()显示环境，使用 env.close()关闭环境。Gym库有对应的[官方文档](https://www.gymlibrary.dev/)，读者可以阅读文档来学习 Gym库 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def decide(self, observation): # 决策\n",
    "        position, velocity = observation\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "        return action # 返回动作\n",
    "\n",
    "    def learn(self, *args): # 学习\n",
    "        pass\n",
    "    \n",
    "\n",
    "def play(env, agent, render=False, train=False):\n",
    "    episode_reward = 0. # 记录回合总奖励，初始化为0\n",
    "    observation = env.reset() # 重置游戏环境，开始新回合\n",
    "    while True: # 不断循环，直到回合结束\n",
    "        if render: # 判断是否显示\n",
    "            env.render() # 显示图形界面，图形界面可以用 env.close() 语句关闭\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action) # 执行动作\n",
    "        episode_reward += reward # 收集回合奖励\n",
    "        if train: # 判断是否训练智能体\n",
    "            agent.learn(observation, action, reward, done) # 学习\n",
    "        if done: # 回合结束，跳出循环\n",
    "            break\n",
    "        observation = next_observation\n",
    "    return episode_reward # 返回回合总奖励\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(3) # 设置随机种子，让结果可复现\n",
    "agent = SimpleAgent(env)\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('观测范围 = {} ~ {}'.format(env.observation_space.low,\n",
    "        env.observation_space.high))\n",
    "print('动作数 = {}'.format(env.action_space.n))\n",
    "\n",
    "episode_reward = play(env, agent, render=True)\n",
    "print('回合奖励 = {}'.format(episode_reward))\n",
    "\n",
    "episode_rewards = [play(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {}'.format(np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Tensorflow和Keras实现的深度强化学习\n",
    "接下来，我们利用Tensorflow和Keras实现一个深度强化学习的项目，在强化学习中，我们需要理解`A-action`,`R-reward`,`E-environment`,`A-agent`这四个基本的概念；\n",
    "接下来，我们会完成一下三件事情：\n",
    "\n",
    "1，用Gym创建一个深度强化学习的环境\n",
    "\n",
    "2，用tensorflow和Keras创建一个深度学习模型\n",
    "\n",
    "3，利用Keras-RL训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装环境\n",
    "首先需要安装以下环境：\n",
    "\n",
    "!pip install tensorflow\n",
    "\n",
    "!pip install keras\n",
    "\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 OpenAI Gym 测试随机环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(actions)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:14.0\n",
      "Episode:2 Score:22.0\n",
      "Episode:3 Score:37.0\n",
      "Episode:4 Score:26.0\n",
      "Episode:5 Score:20.0\n",
      "Episode:6 Score:37.0\n",
      "Episode:7 Score:22.0\n",
      "Episode:8 Score:16.0\n",
      "Episode:9 Score:17.0\n",
      "Episode:10 Score:33.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Keras 创建深度学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                120       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 770 (3.01 KB)\n",
      "Trainable params: 770 (3.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Keras-RL 构建agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## 解决问题ImportError: cannot import name '__version__' from 'tensorflow.keras' \n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_compile_time_distribution_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dqn \u001b[38;5;241m=\u001b[39m build_agent(model, actions)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dqn\u001b[38;5;241m.\u001b[39mfit(env, nb_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gym/lib/python3.8/site-packages/rl/agents/dqn.py:167\u001b[0m, in \u001b[0;36mDQNAgent.compile\u001b[0;34m(self, optimizer, metrics)\u001b[0m\n\u001b[1;32m    164\u001b[0m metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [mean_q]  \u001b[38;5;66;03m# register default metrics\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# We never train the target model, hence we can set the optimizer and loss arbitrarily.\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m \u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_model_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gym/lib/python3.8/site-packages/rl/util.py:16\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, custom_objects)\u001b[0m\n\u001b[1;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mget_config(),\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m clone \u001b[38;5;241m=\u001b[39m model_from_config(config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects)\n\u001b[0;32m---> 16\u001b[0m clone\u001b[38;5;241m.\u001b[39mset_weights(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/Gym/lib/python3.8/site-packages/keras/src/engine/training_v1.py:160\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m        A flat list of Numpy arrays.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     strategy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_time_distribution_strategy\u001b[49m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strategy:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_compile_time_distribution_strategy'"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/cj/miniconda3/envs/Gym/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "2024-04-14 15:20:39.258507: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-04-14 15:20:39.272331: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_5_1/bias/Assign' id:376 op device:{requested: '', assigned: ''} def:{{{node dense_5_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_5_1/bias, dense_5_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute '_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQNAgent(model\u001b[38;5;241m=\u001b[39mmodel, memory\u001b[38;5;241m=\u001b[39mmemory, policy\u001b[38;5;241m=\u001b[39mpolicy, \n\u001b[1;32m     34\u001b[0m                nb_actions\u001b[38;5;241m=\u001b[39mactions, nb_steps_warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, target_model_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 编译DQNAgent\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 训练DQNAgent\u001b[39;00m\n\u001b[1;32m     40\u001b[0m dqn\u001b[38;5;241m.\u001b[39mfit(env, nb_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gym/lib/python3.8/site-packages/rl/agents/dqn.py:175\u001b[0m, in \u001b[0;36mDQNAgent.compile\u001b[0;34m(self, optimizer, metrics)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model_update \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     updates \u001b[38;5;241m=\u001b[39m get_soft_target_model_updates(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model_update)\n\u001b[0;32m--> 175\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdditionalUpdatesOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclipped_masked_error\u001b[39m(args):\n\u001b[1;32m    178\u001b[0m     y_true, y_pred, mask \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/miniconda3/envs/Gym/lib/python3.8/site-packages/rl/util.py:86\u001b[0m, in \u001b[0;36mAdditionalUpdatesOptimizer.__init__\u001b[0;34m(self, optimizer, additional_updates)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, additional_updates):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_updates \u001b[38;5;241m=\u001b[39m additional_updates\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute '_name'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# 定义一个环境\n",
    "# 这里我们使用OpenAI Gym的CartPole环境作为例子\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "# 定义一个模型\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# 定义一个策略\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# 定义一个记忆\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# 创建一个DQNAgent\n",
    "model = build_model(states, actions)\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "               nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "\n",
    "# 编译DQNAgent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# 训练DQNAgent\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍一下Gymnasium\n",
    "Gymnasium是一个项目，为所有单智能体强化学习环境提供 API，并包括常见环境的实现：cartpole（倒立摆）、pendulum（摆锤）、mountain-car（山地车）、mujoco、atari 等等。\n",
    "\n",
    "该 API 包含四个关键函数：make、reset、step 和 render。在 Gymnasium 的核心是 Env。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化环境\n",
    "在 Gymnasium 中初始化环境非常简单，可以通过 make 函数完成：\n",
    "\n",
    "首先，大家需要配置环境，请运行下面的代码；\n",
    "```\n",
    "pip install gymnasium\n",
    "```\n",
    "\n",
    "这还不够，还需要运行：(大家最好加上引号，如果你是Mac并且是zsh,那一定要加上引号)\n",
    "```\n",
    "pip install \"gymnasium[atari]\"\n",
    "```\n",
    "\n",
    "后面的库安装请大家自己学习一下，不会具体的说明了\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将返回一个用于用户交互的 Env。要查看所有可创建的环境，请使用`gymnasium.envs.registry.keys()`。`make` 包括许多额外的参数，用于添加包装器、指定环境的关键字等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与环境进行交互\n",
    "\n",
    "下图是强化学习的简化表示，Gymnasium 实现了这一过程。\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/AE_loop.png\" alt=\"agent-environment loop\" width=\"400\" >\n",
    "\n",
    "这个循环是使用以下 Gymnasium 代码实现的：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要注意的地方\n",
    "大家可能发现，运行上面的代码会出现报错；我们需要安装`swig`; SWIG (Simplified Wrapper and Interface Generator)是一个开源工具，用于连接 C/C++ 与其他高级编程语言（如 Python、Java、C# 等）之间的接口。\n",
    "\n",
    "要安装SWIG，你可以按照以下步骤进行操作：\n",
    "\n",
    "1. **与Python一起安装：**\n",
    "   - 如果你使用的是Python，可以使用pip工具来安装SWIG。在命令行中输入以下命令：\n",
    "     ```\n",
    "     pip install swig\n",
    "     ```\n",
    "   这将会自动安装SWIG并将其与Python关联起来。\n",
    "\n",
    "2. **作为一个独立的可执行文件安装：**\n",
    "   - 如果你需要将SWIG作为一个独立的可执行文件安装，你可以从SWIG的官方网站下载适用于你的操作系统的安装包，并按照安装说明进行安装。\n",
    "   - 请访问SWIG的官方网站（https://www.swig.org/download.html）下载适合你系统的安装程序。\n",
    "\n",
    "无论你选择哪种方法，安装完成后，你就可以在命令行中使用SWIG了。你可以通过输入 `swig -version` 来验证SWIG是否成功安装，并查看其版本信息。\n",
    "\n",
    "我建议第一种方法，如果大家对swig感兴趣，大家可以看看上面的官方网站；不过，大家装好了应该就可以用了；\n",
    "\n",
    "大家可以很好完成下面的安装了\n",
    "```\n",
    "pip install 'gymnasium[box2d]'\n",
    "```\n",
    "\n",
    "ps:如果大家是windows，我建议大家多看看官方文档，需要学会配置swig,不然装了好像也没有用；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
